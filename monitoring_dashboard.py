"""
Promptä¼˜åŒ–ç›‘æ§ä»ªè¡¨æ¿
å®æ—¶ç›‘æ§å’Œè¯„ä¼°Promptæ€§èƒ½æŒ‡æ ‡
"""

import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime, timedelta
import numpy as np
import json
from typing import Dict, List, Any
import time


class MetricsCollector:
    """æŒ‡æ ‡æ”¶é›†å™¨"""
    
    def __init__(self):
        self.metrics_data = {
            "accuracy": [],
            "confidence": [],
            "response_time": [],
            "error_rate": [],
            "user_satisfaction": []
        }
        
    def collect_metric(self, metric_type: str, value: float, metadata: Dict = None):
        """æ”¶é›†å•ä¸ªæŒ‡æ ‡"""
        metric_entry = {
            "timestamp": datetime.now(),
            "value": value,
            "metadata": metadata or {}
        }
        
        if metric_type in self.metrics_data:
            self.metrics_data[metric_type].append(metric_entry)
            
    def get_metrics_df(self, metric_type: str, hours: int = 24) -> pd.DataFrame:
        """è·å–æŒ‡å®šæ—¶é—´èŒƒå›´çš„æŒ‡æ ‡æ•°æ®"""
        if metric_type not in self.metrics_data:
            return pd.DataFrame()
            
        cutoff_time = datetime.now() - timedelta(hours=hours)
        filtered_data = [
            m for m in self.metrics_data[metric_type]
            if m["timestamp"] >= cutoff_time
        ]
        
        if not filtered_data:
            return pd.DataFrame()
            
        df = pd.DataFrame(filtered_data)
        return df


class PromptPerformanceAnalyzer:
    """Promptæ€§èƒ½åˆ†æå™¨"""
    
    def __init__(self):
        self.test_results = []
        self.comparison_results = {}
        
    def analyze_test_results(self, results: List[Dict]) -> Dict:
        """åˆ†ææµ‹è¯•ç»“æœ"""
        analysis = {
            "total_tests": len(results),
            "success_rate": 0,
            "avg_confidence": 0,
            "avg_response_time": 0,
            "error_distribution": {},
            "confidence_distribution": {
                "high": 0,    # >0.8
                "medium": 0,  # 0.6-0.8
                "low": 0      # <0.6
            }
        }
        
        if not results:
            return analysis
            
        # è®¡ç®—æˆåŠŸç‡
        successful_tests = [r for r in results if r.get("success", False)]
        analysis["success_rate"] = len(successful_tests) / len(results)
        
        # è®¡ç®—å¹³å‡ç½®ä¿¡åº¦
        confidences = [r.get("confidence", 0) for r in results]
        analysis["avg_confidence"] = np.mean(confidences)
        
        # è®¡ç®—å¹³å‡å“åº”æ—¶é—´
        response_times = [r.get("response_time", 0) for r in successful_tests]
        if response_times:
            analysis["avg_response_time"] = np.mean(response_times)
        
        # ç½®ä¿¡åº¦åˆ†å¸ƒ
        for conf in confidences:
            if conf > 0.8:
                analysis["confidence_distribution"]["high"] += 1
            elif conf >= 0.6:
                analysis["confidence_distribution"]["medium"] += 1
            else:
                analysis["confidence_distribution"]["low"] += 1
                
        # é”™è¯¯åˆ†å¸ƒ
        errors = [r.get("error_type", "unknown") for r in results if not r.get("success", False)]
        for error in errors:
            analysis["error_distribution"][error] = analysis["error_distribution"].get(error, 0) + 1
            
        return analysis
    
    def compare_prompts(self, old_results: List[Dict], new_results: List[Dict]) -> Dict:
        """å¯¹æ¯”æ–°æ—§Promptæ€§èƒ½"""
        old_analysis = self.analyze_test_results(old_results)
        new_analysis = self.analyze_test_results(new_results)
        
        comparison = {
            "success_rate_improvement": new_analysis["success_rate"] - old_analysis["success_rate"],
            "confidence_improvement": new_analysis["avg_confidence"] - old_analysis["avg_confidence"],
            "response_time_improvement": old_analysis["avg_response_time"] - new_analysis["avg_response_time"],
            "old_analysis": old_analysis,
            "new_analysis": new_analysis
        }
        
        # è®¡ç®—æ”¹è¿›ç™¾åˆ†æ¯”
        if old_analysis["avg_confidence"] > 0:
            comparison["confidence_improvement_pct"] = (
                comparison["confidence_improvement"] / old_analysis["avg_confidence"] * 100
            )
        
        if old_analysis["avg_response_time"] > 0:
            comparison["response_time_improvement_pct"] = (
                comparison["response_time_improvement"] / old_analysis["avg_response_time"] * 100
            )
            
        return comparison


def create_dashboard():
    """åˆ›å»ºStreamlitç›‘æ§ä»ªè¡¨æ¿"""
    
    st.set_page_config(
        page_title="Promptä¼˜åŒ–ç›‘æ§ä»ªè¡¨æ¿",
        page_icon="ğŸ“Š",
        layout="wide"
    )
    
    st.title("ğŸ¯ Promptå·¥ç¨‹ä¼˜åŒ–ç›‘æ§ä»ªè¡¨æ¿")
    st.markdown("---")
    
    # ä¾§è¾¹æ é…ç½®
    with st.sidebar:
        st.header("âš™ï¸ é…ç½®")
        
        # æ—¶é—´èŒƒå›´é€‰æ‹©
        time_range = st.selectbox(
            "æ—¶é—´èŒƒå›´",
            ["æœ€è¿‘1å°æ—¶", "æœ€è¿‘6å°æ—¶", "æœ€è¿‘24å°æ—¶", "æœ€è¿‘7å¤©"],
            index=2
        )
        
        # åˆ·æ–°é¢‘ç‡
        refresh_rate = st.slider(
            "è‡ªåŠ¨åˆ·æ–°é¢‘ç‡ï¼ˆç§’ï¼‰",
            min_value=5,
            max_value=60,
            value=30
        )
        
        # å‘Šè­¦é˜ˆå€¼è®¾ç½®
        st.subheader("ğŸš¨ å‘Šè­¦é˜ˆå€¼")
        confidence_threshold = st.slider(
            "ç½®ä¿¡åº¦å‘Šè­¦é˜ˆå€¼",
            min_value=0.0,
            max_value=1.0,
            value=0.6,
            step=0.05
        )
        
        response_time_threshold = st.slider(
            "å“åº”æ—¶é—´å‘Šè­¦é˜ˆå€¼ï¼ˆç§’ï¼‰",
            min_value=0.5,
            max_value=10.0,
            value=3.0,
            step=0.5
        )
        
        error_rate_threshold = st.slider(
            "é”™è¯¯ç‡å‘Šè­¦é˜ˆå€¼",
            min_value=0.0,
            max_value=0.5,
            value=0.15,
            step=0.05
        )
    
    # ä¸»è¦æŒ‡æ ‡å¡ç‰‡
    st.header("ğŸ“ˆ æ ¸å¿ƒæŒ‡æ ‡")
    col1, col2, col3, col4 = st.columns(4)
    
    # æ¨¡æ‹Ÿæ•°æ®ï¼ˆå®é™…åº”ç”¨ä¸­ä»æ•°æ®åº“è·å–ï¼‰
    current_metrics = {
        "accuracy": 0.76,
        "confidence": 0.82,
        "response_time": 1.8,
        "error_rate": 0.08
    }
    
    with col1:
        st.metric(
            label="å‡†ç¡®ç‡",
            value=f"{current_metrics['accuracy']:.1%}",
            delta="+5.2%",
            delta_color="normal"
        )
        
    with col2:
        st.metric(
            label="å¹³å‡ç½®ä¿¡åº¦",
            value=f"{current_metrics['confidence']:.2f}",
            delta="+0.15",
            delta_color="normal"
        )
        
    with col3:
        st.metric(
            label="å“åº”æ—¶é—´",
            value=f"{current_metrics['response_time']:.1f}ç§’",
            delta="-0.5ç§’",
            delta_color="normal"
        )
        
    with col4:
        st.metric(
            label="é”™è¯¯ç‡",
            value=f"{current_metrics['error_rate']:.1%}",
            delta="-3.2%",
            delta_color="normal"
        )
    
    # è¶‹åŠ¿å›¾
    st.header("ğŸ“Š æ€§èƒ½è¶‹åŠ¿")
    
    # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
    time_points = pd.date_range(
        start=datetime.now() - timedelta(hours=24),
        end=datetime.now(),
        freq='H'
    )
    
    trend_data = pd.DataFrame({
        'timestamp': time_points,
        'confidence': np.random.normal(0.75, 0.1, len(time_points)).clip(0, 1),
        'response_time': np.random.normal(2.0, 0.5, len(time_points)).clip(0.5, 5),
        'error_rate': np.random.normal(0.1, 0.05, len(time_points)).clip(0, 0.3)
    })
    
    col1, col2 = st.columns(2)
    
    with col1:
        # ç½®ä¿¡åº¦è¶‹åŠ¿
        fig_confidence = px.line(
            trend_data,
            x='timestamp',
            y='confidence',
            title='ç½®ä¿¡åº¦è¶‹åŠ¿',
            labels={'confidence': 'ç½®ä¿¡åº¦', 'timestamp': 'æ—¶é—´'}
        )
        fig_confidence.add_hline(
            y=confidence_threshold,
            line_dash="dash",
            line_color="red",
            annotation_text="å‘Šè­¦é˜ˆå€¼"
        )
        st.plotly_chart(fig_confidence, use_container_width=True)
        
    with col2:
        # å“åº”æ—¶é—´è¶‹åŠ¿
        fig_response = px.line(
            trend_data,
            x='timestamp',
            y='response_time',
            title='å“åº”æ—¶é—´è¶‹åŠ¿',
            labels={'response_time': 'å“åº”æ—¶é—´ï¼ˆç§’ï¼‰', 'timestamp': 'æ—¶é—´'}
        )
        fig_response.add_hline(
            y=response_time_threshold,
            line_dash="dash",
            line_color="red",
            annotation_text="å‘Šè­¦é˜ˆå€¼"
        )
        st.plotly_chart(fig_response, use_container_width=True)
    
    # A/Bæµ‹è¯•ç»“æœ
    st.header("ğŸ”¬ A/Bæµ‹è¯•ç»“æœ")
    
    col1, col2 = st.columns(2)
    
    with col1:
        # æ–°æ—§Promptå¯¹æ¯”
        comparison_data = pd.DataFrame({
            'Promptç‰ˆæœ¬': ['æ—§ç‰ˆæœ¬', 'æ–°ç‰ˆæœ¬'],
            'å‡†ç¡®ç‡': [0.68, 0.76],
            'ç½®ä¿¡åº¦': [0.67, 0.82],
            'å“åº”æ—¶é—´': [2.3, 1.8]
        })
        
        fig_comparison = go.Figure(data=[
            go.Bar(name='å‡†ç¡®ç‡', x=comparison_data['Promptç‰ˆæœ¬'], y=comparison_data['å‡†ç¡®ç‡']),
            go.Bar(name='ç½®ä¿¡åº¦', x=comparison_data['Promptç‰ˆæœ¬'], y=comparison_data['ç½®ä¿¡åº¦'])
        ])
        fig_comparison.update_layout(
            title='æ–°æ—§Promptæ€§èƒ½å¯¹æ¯”',
            barmode='group',
            yaxis_title='å€¼'
        )
        st.plotly_chart(fig_comparison, use_container_width=True)
        
    with col2:
        # é—®é¢˜ç±»å‹æ€§èƒ½åˆ†å¸ƒ
        question_types = ['åŸºç¡€ä¿¡æ¯', 'æ•°å€¼å‚æ•°', 'æ¦‚å¿µç†è§£', 'å¯¹æ¯”åˆ†æ']
        performance_by_type = {
            'å‡†ç¡®ç‡': [0.85, 0.78, 0.72, 0.69],
            'ç½®ä¿¡åº¦': [0.90, 0.83, 0.75, 0.71]
        }
        
        fig_radar = go.Figure()
        fig_radar.add_trace(go.Scatterpolar(
            r=performance_by_type['å‡†ç¡®ç‡'],
            theta=question_types,
            fill='toself',
            name='å‡†ç¡®ç‡'
        ))
        fig_radar.add_trace(go.Scatterpolar(
            r=performance_by_type['ç½®ä¿¡åº¦'],
            theta=question_types,
            fill='toself',
            name='ç½®ä¿¡åº¦'
        ))
        fig_radar.update_layout(
            polar=dict(
                radialaxis=dict(
                    visible=True,
                    range=[0, 1]
                )),
            showlegend=True,
            title="é—®é¢˜ç±»å‹æ€§èƒ½åˆ†å¸ƒ"
        )
        st.plotly_chart(fig_radar, use_container_width=True)
    
    # é”™è¯¯åˆ†æ
    st.header("âš ï¸ é”™è¯¯åˆ†æ")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        # é”™è¯¯ç±»å‹åˆ†å¸ƒ
        error_types = pd.DataFrame({
            'é”™è¯¯ç±»å‹': ['æ ¼å¼é”™è¯¯', 'å†…å®¹é”™è¯¯', 'ä¸Šä¸‹æ–‡ç¼ºå¤±', 'æŒ‡ä»¤ä¸æ˜'],
            'æ•°é‡': [15, 28, 12, 8]
        })
        
        fig_error = px.pie(
            error_types,
            values='æ•°é‡',
            names='é”™è¯¯ç±»å‹',
            title='é”™è¯¯ç±»å‹åˆ†å¸ƒ'
        )
        st.plotly_chart(fig_error, use_container_width=True)
        
    with col2:
        # ç½®ä¿¡åº¦åˆ†å¸ƒ
        confidence_dist = pd.DataFrame({
            'ç½®ä¿¡åº¦åŒºé—´': ['é«˜(>0.8)', 'ä¸­(0.6-0.8)', 'ä½(<0.6)'],
            'æ•°é‡': [145, 82, 23]
        })
        
        fig_conf_dist = px.bar(
            confidence_dist,
            x='ç½®ä¿¡åº¦åŒºé—´',
            y='æ•°é‡',
            title='ç½®ä¿¡åº¦åˆ†å¸ƒ',
            color='ç½®ä¿¡åº¦åŒºé—´',
            color_discrete_map={
                'é«˜(>0.8)': 'green',
                'ä¸­(0.6-0.8)': 'yellow',
                'ä½(<0.6)': 'red'
            }
        )
        st.plotly_chart(fig_conf_dist, use_container_width=True)
        
    with col3:
        # å‘Šè­¦çŠ¶æ€
        st.subheader("ğŸš¨ å½“å‰å‘Šè­¦")
        
        # æ£€æŸ¥å‘Šè­¦æ¡ä»¶
        alerts = []
        if current_metrics['confidence'] < confidence_threshold:
            alerts.append(f"âš ï¸ ç½®ä¿¡åº¦ä½äºé˜ˆå€¼: {current_metrics['confidence']:.2f} < {confidence_threshold}")
        if current_metrics['response_time'] > response_time_threshold:
            alerts.append(f"âš ï¸ å“åº”æ—¶é—´è¶…è¿‡é˜ˆå€¼: {current_metrics['response_time']:.1f}ç§’ > {response_time_threshold}ç§’")
        if current_metrics['error_rate'] > error_rate_threshold:
            alerts.append(f"âš ï¸ é”™è¯¯ç‡è¶…è¿‡é˜ˆå€¼: {current_metrics['error_rate']:.1%} > {error_rate_threshold:.1%}")
            
        if alerts:
            for alert in alerts:
                st.warning(alert)
        else:
            st.success("âœ… æ‰€æœ‰æŒ‡æ ‡æ­£å¸¸")
    
    # è¯¦ç»†æ•°æ®è¡¨æ ¼
    st.header("ğŸ“‹ è¯¦ç»†æµ‹è¯•ç»“æœ")
    
    # ç”Ÿæˆæ¨¡æ‹Ÿæµ‹è¯•æ•°æ®
    test_data = pd.DataFrame({
        'æ—¶é—´': pd.date_range(start=datetime.now() - timedelta(hours=1), periods=20, freq='3min'),
        'é—®é¢˜': [f"é—®é¢˜{i}" for i in range(1, 21)],
        'ç±»å‹': np.random.choice(['åŸºç¡€ä¿¡æ¯', 'æ•°å€¼å‚æ•°', 'æ¦‚å¿µç†è§£', 'å¯¹æ¯”åˆ†æ'], 20),
        'ç½®ä¿¡åº¦': np.random.uniform(0.5, 1.0, 20).round(3),
        'å“åº”æ—¶é—´(ç§’)': np.random.uniform(0.5, 4.0, 20).round(2),
        'çŠ¶æ€': np.random.choice(['æˆåŠŸ', 'å¤±è´¥'], 20, p=[0.9, 0.1])
    })
    
    # æ·»åŠ çŠ¶æ€é¢œè‰²
    def highlight_status(row):
        if row['çŠ¶æ€'] == 'å¤±è´¥':
            return ['background-color: #ffcccc'] * len(row)
        elif row['ç½®ä¿¡åº¦'] < 0.6:
            return ['background-color: #ffffcc'] * len(row)
        else:
            return [''] * len(row)
    
    styled_df = test_data.style.apply(highlight_status, axis=1)
    st.dataframe(styled_df, use_container_width=True)
    
    # ä¼˜åŒ–å»ºè®®
    st.header("ğŸ’¡ ä¼˜åŒ–å»ºè®®")
    
    suggestions = []
    
    if current_metrics['confidence'] < 0.7:
        suggestions.append("â€¢ å¢åŠ Few-shotç¤ºä¾‹ï¼Œæé«˜æ¨¡å‹å¯¹ä»»åŠ¡çš„ç†è§£")
        suggestions.append("â€¢ ä¼˜åŒ–Promptç»“æ„ï¼Œä½¿ç”¨æ›´æ¸…æ™°çš„æŒ‡ä»¤")
        suggestions.append("â€¢ æ·»åŠ é¢†åŸŸçŸ¥è¯†æ³¨å…¥ï¼Œæé«˜ä¸“ä¸šæœ¯è¯­ç†è§£")
        
    if current_metrics['response_time'] > 2.5:
        suggestions.append("â€¢ ç®€åŒ–Prompté•¿åº¦ï¼Œå‡å°‘å¤„ç†æ—¶é—´")
        suggestions.append("â€¢ ä½¿ç”¨ç¼“å­˜æœºåˆ¶ï¼Œé¿å…é‡å¤è®¡ç®—")
        suggestions.append("â€¢ è€ƒè™‘ä½¿ç”¨æ›´å°çš„æ¨¡å‹è¿›è¡Œé¢„ç­›é€‰")
        
    if current_metrics['error_rate'] > 0.1:
        suggestions.append("â€¢ åŠ å¼ºè¾“å…¥éªŒè¯ï¼Œå‡å°‘æ ¼å¼é”™è¯¯")
        suggestions.append("â€¢ å®Œå–„é”™è¯¯å¤„ç†æœºåˆ¶")
        suggestions.append("â€¢ å¢åŠ é‡è¯•é€»è¾‘å’Œé™çº§ç­–ç•¥")
    
    if suggestions:
        for suggestion in suggestions:
            st.info(suggestion)
    else:
        st.success("å½“å‰æ€§èƒ½è‰¯å¥½ï¼Œç»§ç»­ä¿æŒï¼")
    
    # è‡ªåŠ¨åˆ·æ–°
    st.markdown("---")
    st.caption(f"æœ€åæ›´æ–°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} | è‡ªåŠ¨åˆ·æ–°: {refresh_rate}ç§’")
    
    # æ·»åŠ è‡ªåŠ¨åˆ·æ–°åŠŸèƒ½
    time.sleep(refresh_rate)
    st.rerun()


if __name__ == "__main__":
    create_dashboard()