"""
ä¿é™©äº§å“RAGç³»ç»Ÿ - HuggingFace Spaceså…è´¹éƒ¨ç½²ç‰ˆæœ¬
ä½¿ç”¨Gradioç•Œé¢ï¼Œå®Œå…¨å…è´¹æ‰˜ç®¡
"""
import gradio as gr
import os
from typing import List, Dict, Tuple
import json
from datetime import datetime
import pandas as pd
import pypdf2
import pdfplumber
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI

# å…¨å±€å˜é‡
vector_store = None
qa_history = []
documents = []

# é¢„è®¾é—®é¢˜
PRESET_QUESTIONS = [
    "What is the minimum premium?",
    "What is the policy currency?",
    "What is the minimum issue age?",
    "What is the maximum issue age?",
    "What is the benefit term?",
    "What are the premium payment terms available?",
    "What premium payment modes are available?",
    "What is the minimum modal premium?",
    "What is the surrender value?",
    "What are the fees and charges?",
    "What is the free-look period?",
    "What are the exclusions?",
    "What are the riders available?",
    "What is the grace period for premium payment?",
    "What are the death benefit options?",
]

def process_pdf(pdf_file) -> str:
    """å¤„ç†ä¸Šä¼ çš„PDFæ–‡ä»¶"""
    global documents, vector_store
    
    if pdf_file is None:
        return "è¯·å…ˆä¸Šä¼ PDFæ–‡ä»¶"
    
    try:
        # æå–PDFæ–‡æœ¬
        text = ""
        with pdfplumber.open(pdf_file.name) as pdf:
            for page_num, page in enumerate(pdf.pages, 1):
                page_text = page.extract_text() or ""
                text += f"\n[Page {page_num}]\n{page_text}\n"
                
                # æå–è¡¨æ ¼
                tables = page.extract_tables()
                for table in tables:
                    if table:
                        df = pd.DataFrame(table[1:], columns=table[0])
                        text += f"\n[Table on Page {page_num}]\n{df.to_string()}\n"
        
        # æ–‡æœ¬åˆ†å—
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=800,
            chunk_overlap=200
        )
        chunks = text_splitter.split_text(text)
        
        # åˆ›å»ºæ–‡æ¡£
        documents = [{"content": chunk, "metadata": {"source": os.path.basename(pdf_file.name)}} 
                    for chunk in chunks]
        
        # åˆ›å»ºå‘é‡å­˜å‚¨
        embeddings = OpenAIEmbeddings()
        texts = [doc["content"] for doc in documents]
        metadatas = [doc["metadata"] for doc in documents]
        
        vector_store = FAISS.from_texts(
            texts=texts,
            embedding=embeddings,
            metadatas=metadatas
        )
        
        return f"âœ… æˆåŠŸå¤„ç†PDF: {os.path.basename(pdf_file.name)}\nå…±åˆ›å»º {len(chunks)} ä¸ªæ–‡æœ¬å—"
    
    except Exception as e:
        return f"âŒ å¤„ç†PDFæ—¶å‡ºé”™: {str(e)}"

def answer_question(question: str, api_key: str) -> Tuple[str, str]:
    """å›ç­”å•ä¸ªé—®é¢˜"""
    global vector_store, qa_history
    
    if not api_key:
        return "è¯·å…ˆè¾“å…¥OpenAI APIå¯†é’¥", ""
    
    if vector_store is None:
        return "è¯·å…ˆä¸Šä¼ å¹¶å¤„ç†PDFæ–‡æ¡£", ""
    
    if not question:
        return "è¯·è¾“å…¥é—®é¢˜", ""
    
    try:
        os.environ["OPENAI_API_KEY"] = api_key
        
        llm = ChatOpenAI(
            model_name="gpt-3.5-turbo",
            temperature=0.1,
            max_tokens=500
        )
        
        qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=vector_store.as_retriever(search_kwargs={"k": 5}),
            return_source_documents=True
        )
        
        result = qa_chain({"query": question})
        answer = result["result"]
        sources = [doc.metadata.get("source", "Unknown") for doc in result.get("source_documents", [])][:3]
        
        # ä¿å­˜åˆ°å†å²
        qa_history.append({
            "question": question,
            "answer": answer,
            "sources": sources,
            "timestamp": datetime.now().isoformat()
        })
        
        sources_text = f"\n\nğŸ“š æ¥æº: {', '.join(set(sources))}" if sources else ""
        
        return answer, sources_text
    
    except Exception as e:
        return f"âŒ é”™è¯¯: {str(e)}", ""

def batch_answer(api_key: str, progress=gr.Progress()) -> str:
    """æ‰¹é‡å›ç­”é¢„è®¾é—®é¢˜"""
    global vector_store
    
    if not api_key:
        return "è¯·å…ˆè¾“å…¥OpenAI APIå¯†é’¥"
    
    if vector_store is None:
        return "è¯·å…ˆä¸Šä¼ å¹¶å¤„ç†PDFæ–‡æ¡£"
    
    results = []
    for i, question in enumerate(progress.tqdm(PRESET_QUESTIONS, desc="å¤„ç†é—®é¢˜")):
        answer, sources = answer_question(question, api_key)
        results.append(f"**Q{i+1}: {question}**\n{answer}\n")
    
    return "\n---\n".join(results)

def export_results() -> Tuple[str, str]:
    """å¯¼å‡ºé—®ç­”ç»“æœ"""
    global qa_history
    
    if not qa_history:
        return None, None
    
    # ç”ŸæˆCSV
    df = pd.DataFrame(qa_history)
    csv_path = f"qa_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
    df.to_csv(csv_path, index=False)
    
    # ç”ŸæˆJSON
    json_path = f"qa_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(qa_history, f, ensure_ascii=False, indent=2)
    
    return csv_path, json_path

def create_interface():
    """åˆ›å»ºGradioç•Œé¢"""
    with gr.Blocks(title="ä¿é™©äº§å“RAGç³»ç»Ÿ", theme=gr.themes.Soft()) as demo:
        gr.Markdown("""
        # ğŸ¥ ä¿é™©äº§å“æ™ºèƒ½é—®ç­”ç³»ç»Ÿ
        ### åŸºäºRAGæŠ€æœ¯çš„ä¿é™©æ–‡æ¡£æ™ºèƒ½åˆ†æ - å…è´¹æ‰˜ç®¡ç‰ˆæœ¬
        """)
        
        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("### âš™ï¸ é…ç½®")
                api_key = gr.Textbox(
                    label="OpenAI API Key",
                    type="password",
                    placeholder="sk-...",
                    info="æ‚¨çš„APIå¯†é’¥ä»…åœ¨ä¼šè¯ä¸­ä½¿ç”¨ï¼Œä¸ä¼šè¢«å­˜å‚¨"
                )
                
                gr.Markdown("### ğŸ“„ æ–‡æ¡£ä¸Šä¼ ")
                pdf_file = gr.File(
                    label="ä¸Šä¼ PDFæ–‡ä»¶",
                    file_types=[".pdf"],
                    type="filepath"
                )
                
                process_btn = gr.Button("ğŸ”„ å¤„ç†æ–‡æ¡£", variant="primary")
                process_status = gr.Textbox(label="å¤„ç†çŠ¶æ€", interactive=False)
                
                gr.Markdown("### ğŸ“Š å¿«é€Ÿæ“ä½œ")
                batch_btn = gr.Button("ğŸš€ æ‰¹é‡å›ç­”é¢„è®¾é—®é¢˜", variant="secondary")
                export_btn = gr.Button("ğŸ’¾ å¯¼å‡ºç»“æœ", variant="secondary")
                
            with gr.Column(scale=2):
                gr.Markdown("### ğŸ’¬ é—®ç­”ç•Œé¢")
                
                with gr.Tab("è‡ªå®šä¹‰é—®é¢˜"):
                    custom_question = gr.Textbox(
                        label="è¾“å…¥æ‚¨çš„é—®é¢˜",
                        placeholder="ä¾‹å¦‚: What is the minimum premium?",
                        lines=2
                    )
                    custom_submit = gr.Button("ğŸ” è·å–ç­”æ¡ˆ")
                    
                with gr.Tab("é¢„è®¾é—®é¢˜"):
                    preset_question = gr.Dropdown(
                        label="é€‰æ‹©é¢„è®¾é—®é¢˜",
                        choices=PRESET_QUESTIONS,
                        value=PRESET_QUESTIONS[0]
                    )
                    preset_submit = gr.Button("ğŸ” è·å–ç­”æ¡ˆ")
                
                answer_output = gr.Textbox(
                    label="ç­”æ¡ˆ",
                    lines=6,
                    interactive=False
                )
                
                sources_output = gr.Textbox(
                    label="æ¥æº",
                    lines=2,
                    interactive=False
                )
                
                with gr.Tab("æ‰¹é‡ç»“æœ"):
                    batch_output = gr.Markdown()
                
                with gr.Tab("å¯¼å‡º"):
                    csv_file = gr.File(label="CSVæ–‡ä»¶")
                    json_file = gr.File(label="JSONæ–‡ä»¶")
        
        # äº‹ä»¶ç»‘å®š
        process_btn.click(
            fn=process_pdf,
            inputs=[pdf_file],
            outputs=[process_status]
        )
        
        custom_submit.click(
            fn=answer_question,
            inputs=[custom_question, api_key],
            outputs=[answer_output, sources_output]
        )
        
        preset_submit.click(
            fn=answer_question,
            inputs=[preset_question, api_key],
            outputs=[answer_output, sources_output]
        )
        
        batch_btn.click(
            fn=batch_answer,
            inputs=[api_key],
            outputs=[batch_output]
        )
        
        export_btn.click(
            fn=export_results,
            inputs=[],
            outputs=[csv_file, json_file]
        )
        
        # ç¤ºä¾‹
        gr.Examples(
            examples=[
                ["What is the minimum premium?"],
                ["What are the death benefit options?"],
                ["What is the surrender value?"],
                ["What are the fees and charges?"],
            ],
            inputs=[custom_question]
        )
        
        gr.Markdown("""
        ---
        ### ğŸ“– ä½¿ç”¨è¯´æ˜
        1. è¾“å…¥OpenAI APIå¯†é’¥
        2. ä¸Šä¼ ä¿é™©äº§å“PDFæ–‡ä»¶
        3. ç‚¹å‡»"å¤„ç†æ–‡æ¡£"æŒ‰é’®
        4. è¾“å…¥é—®é¢˜æˆ–é€‰æ‹©é¢„è®¾é—®é¢˜
        5. è·å–AIç”Ÿæˆçš„ç­”æ¡ˆ
        
        ### ğŸŒŸ ç‰¹ç‚¹
        - å®Œå…¨å…è´¹æ‰˜ç®¡åœ¨HuggingFace Spaces
        - æ”¯æŒä¸­è‹±æ–‡PDFæ–‡æ¡£
        - è‡ªåŠ¨æå–è¡¨æ ¼æ•°æ®
        - æ‰¹é‡é—®ç­”åŠŸèƒ½
        - ç»“æœå¯¼å‡ºåŠŸèƒ½
        """)
    
    return demo

# å¯åŠ¨åº”ç”¨
if __name__ == "__main__":
    demo = create_interface()
    demo.launch(
        share=False,
        server_name="0.0.0.0",
        server_port=7860
    )